local M = {}
local config = require("llm.config")
local Job = require("plenary.job")
local ns = vim.api.nvim_create_namespace("llm")

local function get_api_key(name)
	return os.getenv(name)
end

function M.get_lines_to_cursor()
	local current_buffer = vim.api.nvim_get_current_buf()
	local current_window = vim.api.nvim_get_current_win()
	local current_pos = vim.api.nvim_win_get_cursor(current_window)
	local row = current_pos[1]

	local lines = vim.api.nvim_buf_get_lines(current_buffer, 0, row, true)

	return table.concat(lines, "\n")
end

function M.get_visual_selection()
	local _, srow, scol = unpack(vim.fn.getpos("v"))
	local _, erow, ecol = unpack(vim.fn.getpos("."))

	if vim.fn.mode() == "V" then
		if srow > erow then
			srow, erow = erow, srow
		end
		return vim.api.nvim_buf_get_lines(0, srow - 1, erow, true)
	end

	if vim.fn.mode() == "v" then
		if srow < erow or (srow == erow and scol <= ecol) then
			return vim.api.nvim_buf_get_text(0, srow - 1, scol - 1, erow - 1, ecol, {})
		else
			return vim.api.nvim_buf_get_text(0, erow - 1, ecol - 1, srow - 1, scol, {})
		end
	end

	if vim.fn.mode() == "\22" then
		local lines = {}
		if srow > erow then
			srow, erow = erow, srow
		end

		if scol > ecol then
			scol, ecol = ecol, scol
		end

		for i = srow, erow do
			table.insert(lines, vim.api.nvim_buf_get_text(0, i - 1, scol - 1, i - 1, ecol, {})[1])
		end
		return lines
	end
end

function M.make_anthropic_spec_curl_args(opts, prompt, system_prompt)
	local url = opts.url
	local api_key = opts.api_key_name and get_api_key(opts.api_key_name)

	local data = {
		system = system_prompt,
		messages = { { role = "user", content = prompt } },
		model = opts.model,
		temperature = 0.7,
		stream = true,
		max_tokens = 4096,
	}

	local args = { "-N", "-X", "POST", "-H", "Content-Type: application/json", "-d", vim.json.encode(data) }

	if api_key then
		table.insert(args, "-H")
		table.insert(args, "x-api-key: " .. api_key)
		table.insert(args, "-H")
		table.insert(args, "anthropic-version: 2023-06-01")
	end
	table.insert(args, url)
	return args
end

function M.make_openai_spec_curl_args(opts, prompt, system_prompt)
	local url = opts.url
	local api_key = opts.api_key_name and get_api_key(opts.api_key_name)

	local data = {
		messages = { { role = "system", content = system_prompt }, { role = "user", content = prompt } },
		model = opts.model,
		temperature = 0.7,
		stream = true,
	}

	local args = { "-N", "-X", "POST", "-H", "Content-Type: application/json", "-d", vim.json.encode(data) }

	if api_key then
		table.insert(args, "-H")
		table.insert(args, "Authorization: Bearer " .. api_key)
	end
	table.insert(args, url)
	return args
end

-- Function to open the floating window with selected text and a specific model
function M.open_hover_window_with_model(model)
	-- Get selected text in visual mode
	local selected_text = M.get_visual_selection()
	if not selected_text or selected_text == "" then
		vim.notify("No text selected", vim.log.levels.WARN)
		return
	end

	vim.notify("Selected text: " .. selected_text, vim.log.levels.DEBUG)

	-- Create a new buffer
	local buf = vim.api.nvim_create_buf(false, true) -- (listed, scratch)
	if not buf then
		vim.notify("Failed to create buffer.", vim.log.levels.ERROR)
		return
	end
	M.state.buf = buf

	-- Define window dimensions
	local width = math.floor(vim.o.columns * 0.8)
	local height = math.floor(vim.o.lines * 0.6)
	local row = math.floor((vim.o.lines - height) / 2 - 1)
	local col = math.floor((vim.o.columns - width) / 2)

	-- Window options
	local opts = {
		style = "minimal",
		relative = "editor",
		width = width,
		height = height,
		row = row,
		col = col,
	}

	-- Create floating window
	local success, win = pcall(vim.api.nvim_open_win, buf, true, opts)
	if not success then
		vim.notify("Failed to open floating window.", vim.log.levels.ERROR)
		return
	end
	M.state.win = win

	-- Insert prompt and selected text
	local prompt = config.get_prompt()
	local initial_content = prompt .. "\n\n" .. selected_text
	vim.api.nvim_buf_set_lines(buf, 0, -1, false, vim.split(initial_content, "\n"))

	-- Set filetype for better syntax highlighting (optional)
	vim.api.nvim_buf_set_option(buf, "filetype", "markdown") -- Example filetype

	-- Store the selected model
	M.state.selected_model = model

	vim.notify("Floating window created successfully.", vim.log.levels.INFO)

	-- Set keybindings inside the window
	M.set_window_keybindings(buf)
end

-- Function to open the floating window with the default model
function M.open_hover_window()
	local default_model = config.get_selected_model()
	vim.notify("Opening hover window with default model: " .. default_model, vim.log.levels.INFO)
	M.open_hover_window_with_model(default_model)
end

-- Function to set keybindings inside the floating window
function M.set_window_keybindings(buf)
	vim.notify("Setting keybindings in floating window.", vim.log.levels.DEBUG)
	-- <C-s> to send to LLM
	vim.api.nvim_buf_set_keymap(
		buf,
		"n",
		"<C-s>",
		':lua require("llm.api").send_to_llm()<CR>',
		{ noremap = true, silent = true }
	)

	-- <C-c> to close the window
	vim.api.nvim_buf_set_keymap(
		buf,
		"n",
		"<C-c>",
		':lua require("llm.api").close_hover_window()<CR>',
		{ noremap = true, silent = true }
	)
end

-- Function to send buffer content to LLM
function M.send_to_llm()
	vim.notify("Sending content to LLM...", vim.log.levels.INFO)
	local buf = M.state.buf
	if not buf or not vim.api.nvim_buf_is_loaded(buf) then
		vim.notify("No active LLM window.", vim.log.levels.ERROR)
		return
	end

	-- Get the content from the buffer
	local lines = vim.api.nvim_buf_get_lines(buf, 0, -1, false)
	local content = table.concat(lines, "\n")

	vim.notify("Buffer content retrieved.", vim.log.levels.DEBUG)

	-- Split content into prompt and user text
	local prompt, user_text = content:match("^(.-)\n\n(.*)$")
	if not prompt or not user_text or user_text == "" then
		vim.notify("Invalid format. Ensure prompt and text are separated by two newlines.", vim.log.levels.ERROR)
		return
	end

	vim.notify("Prompt: " .. prompt, vim.log.levels.DEBUG)
	vim.notify("User Text: " .. user_text, vim.log.levels.DEBUG)

	-- Get the selected LLM model
	local model = M.state.selected_model or config.get_selected_model()
	vim.notify("Using model: " .. model, vim.log.levels.INFO)

	-- Send request to Ollama's /api/generate endpoint asynchronously using plenary.job with curl
	M.call_llm_api(model, prompt, user_text, function(response)
		vim.notify("LLM responded. Inserting response...", vim.log.levels.INFO)
		-- Insert response into the buffer
		vim.schedule(function()
			-- Find the index to insert response
			local response_header = "Response:"
			local existing = vim.fn.search(response_header, "nw")
			if existing == 0 then
				vim.api.nvim_buf_set_lines(buf, -1, -1, false, { "", response_header, response })
				vim.notify("Response inserted after header.", vim.log.levels.INFO)
			else
				vim.api.nvim_buf_set_lines(buf, existing + 1, existing + 1, false, { response })
				vim.notify("Response appended.", vim.log.levels.INFO)
			end
		end)
	end)
end

-- Function to make API call to Ollama's /api/generate using plenary.job with curl
function M.call_llm_api(model, prompt, user_text, callback)
	vim.notify("Preparing API request...", vim.log.levels.DEBUG)
	-- Retrieve model configuration
	local model_config = config.get_model_config(model)
	if not model_config then
		vim.notify("Model configuration for '" .. model .. "' not found.", vim.log.levels.ERROR)
		return
	end

	local url = model_config.url

	if not url then
		vim.notify("API URL for model '" .. model .. "' is not configured.", vim.log.levels.ERROR)
		return
	end

	-- Prepare the request body
	local request_body = vim.json.encode({
		model = model_config.model_name,
		prompt = prompt .. "\n" .. user_text,
		stream = false, -- Disable streaming
		suffix = model_config.suffix,
		options = model_config.options,
	})

	vim.notify("Request Body: " .. request_body, vim.log.levels.DEBUG)

	-- Prepare headers
	local headers = {
		"Content-Type: application/json",
	}
	vim.notify("Request URL: " .. url, vim.log.levels.DEBUG)

	-- Use plenary.job to execute curl asynchronously
	Job:new({
		command = "curl",
		args = {
			"-s", -- Silent mode
			"-X",
			"POST",
			"-d",
			request_body,
			unpack(headers),
			url,
		},
		on_exit = function(j, return_val)
			vim.schedule(function()
				vim.notify("Curl process exited with code: " .. return_val, vim.log.levels.DEBUG)
			end)
			if return_val ~= 0 then
				vim.schedule(function()
					vim.notify("Ollama API request failed.", vim.log.levels.ERROR)
				end)
				return
			end

			local response = table.concat(j:result(), "\n")
			vim.schedule(function()
				vim.notify("Raw API Response: " .. response, vim.log.levels.DEBUG)
			end)

			local success, parsed = pcall(vim.json.decode, response)

			if not success then
				vim.schedule(function()
					vim.notify("Failed to parse JSON response: " .. parsed, vim.log.levels.ERROR)
				end)
				return
			end

			-- Extract the 'response' field from the API response
			if parsed.response and parsed.response ~= "" then
				callback(parsed.response)
			else
				vim.schedule(function()
					vim.notify("No response text received from Ollama API.", vim.log.levels.ERROR)
				end)
			end
		end,
	}):start()

	vim.notify("API call initiated.", vim.log.levels.INFO)
end

-- Function to close the floating window
function M.close_hover_window()
	vim.notify("Attempting to close hover window...", vim.log.levels.INFO)
	if M.state.win and vim.api.nvim_win_is_valid(M.state.win) then
		vim.api.nvim_win_close(M.state.win, true)
		vim.notify("Floating window closed.", vim.log.levels.INFO)
		M.state.win = nil
		M.state.buf = nil
		M.state.selected_model = nil
	else
		vim.notify("No active LLM window to close.", vim.log.levels.WARN)
	end
end

return M
